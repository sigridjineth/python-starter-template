{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Chat with Storm API\n",
    "\n",
    "Learn how to chat with your documents using Storm's RAG capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [requests]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 requests-2.32.4 urllib3-2.5.0\n",
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "API_KEY = os.getenv(\"STORM_API_KEY\", \"st_fd22b63dc2304bcc9da18744ca462631\")\n",
    "API_URL = \"https://live-stargate.sionic.im\"\n",
    "\n",
    "headers = {\"storm-api-key\": API_KEY}\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Chat Request\n",
    "\n",
    "Send a question and get an AI-powered answer based on your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Question: can you promote Sea World Bangkok Ocean world the Riverfront to young students?\n",
      "\n",
      "üí¨ Answer: \"üåäüê† ‡∏°‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏•‡∏Å‡πÉ‡∏ï‡πâ‡∏ó‡∏∞‡πÄ‡∏•‡∏ó‡∏µ‡πà Sea Life Bangkok Ocean World ‡∏Å‡∏±‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞! ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ô‡∏ß‡∏õ‡∏∞‡∏Å‡∏≤‡∏£‡∏±‡∏á‡∏™‡∏µ‡∏™‡∏±‡∏ô‡∏™‡∏î‡πÉ‡∏™ ‡∏≠‡∏∏‡πÇ‡∏°‡∏á‡∏Ñ‡πå‡πÉ‡∏ï‡πâ‡∏ó‡∏∞‡πÄ‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ‡∏ô‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏ä‡∏≤‡∏£‡πå‡∏Ñ ‡∏ß‡∏≠‡∏•‡πå‡∏Ñ‡∏ó‡∏µ‡πà‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏™‡∏∏‡∏î‡πÜ! ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≠‡∏á‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡∏∏‡∏Å‡πÑ‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô! ‡πÄ‡∏õ‡∏¥‡∏î‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô 10.00 - 20.00 ‡∏ô. #SeaLifeAdventure #‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏™‡∏ô‡∏∏‡∏Å‡∏™‡∏ô‡∏≤‡∏ô\"\n",
      "\n",
      "üìö Used 91 sources\n",
      "üßµ Thread ID: 7361670852860026880\n",
      "‚úÖ Status: success\n"
     ]
    }
   ],
   "source": [
    "def chat_with_documents(query, variables=None):\n",
    "    \"\"\"Send a chat request to Storm API.\"\"\"\n",
    "    \n",
    "    data = {\"question\": query}\n",
    "    if variables:\n",
    "        data[\"variables\"] = variables\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/api/v2/answer\",\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()[\"data\"]\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Test with a simple question\n",
    "question = \"can you promote Sea World Bangkok Ocean world the Riverfront to young students?\"\n",
    "print(f\"ü§î Question: {question}\\n\")\n",
    "\n",
    "result = chat_with_documents(question)\n",
    "\n",
    "if result:\n",
    "    chat = result[\"chat\"]\n",
    "    contexts = result[\"contexts\"]\n",
    "    \n",
    "    print(f\"üí¨ Answer: {chat['answer']}\\n\")\n",
    "    print(f\"üìö Used {len(contexts)} sources\")\n",
    "    print(f\"üßµ Thread ID: {chat['threadId']}\")\n",
    "    print(f\"‚úÖ Status: {chat['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Context Sources\n",
    "\n",
    "Storm API provides context sources for each answer - the documents and snippets used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_contexts(contexts):\n",
    "    \"\"\"Display context sources in a readable format.\"\"\"\n",
    "    \n",
    "    if not contexts:\n",
    "        print(\"No contexts provided\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìö Context Sources ({len(contexts)} total):\\n\")\n",
    "    \n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        print(f\"Source {i}:\")\n",
    "        print(f\"  üì¶ Bucket: {ctx['bucketName']}\")\n",
    "        print(f\"  üìÑ File: {ctx['fileName']}\")\n",
    "        print(f\"  üìë Page: {ctx['pageName']}\")\n",
    "        print(f\"  üî¢ Reference: [{ctx['referenceIdx']}]\")\n",
    "        print(f\"  üìù Type: {ctx['type']}\")\n",
    "        print(f\"\\n  Context snippet:\")\n",
    "        print(f\"  \\\"{ctx['context'][:200]}...\\\"\" if len(ctx['context']) > 200 else f\"  \\\"{ctx['context']}\\\"\")\n",
    "        print()\n",
    "\n",
    "# Show contexts from previous answer\n",
    "if 'result' in locals() and result:\n",
    "    display_contexts(result['contexts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conversation Threading\n",
    "\n",
    "Maintain conversation context across multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Starting conversation...\n",
      "\n",
      "Q: What is RAG?\n",
      "A: RAG ‡∏´‡∏£‡∏∑‡∏≠ Retrieval-Augmented Generation ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Retrieval) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Generation) ‡πÄ‡∏Ç...\n",
      "üìö Sources: 79\n",
      "\n",
      "Q: How does it work?\n",
      "A: ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á TikTok Commerce Assistant Bot ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≤‡πÅ‡∏£‡∏á‡∏ö‡∏ô TikTok ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Ç...\n",
      "üìö Sources: 84\n",
      "\n",
      "Q: What are the benefits?\n",
      "A: ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø ‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏ö‡πÄ‡∏•‡∏¢! üåü ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡∏ä‡∏¥‡∏°‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏™‡∏ï‡∏£‡∏µ‡∏ó‡∏ü‡∏π‡πâ‡∏î‡∏™‡∏∏‡∏î‡∏ü‡∏¥‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏¥‡πâ‡∏á‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡∏ô‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤...\n",
      "üìö Sources: 88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ChatSession:\n",
    "    \"\"\"Manage a chat session with Storm API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, api_url=\"https://live-stargate.sionic.im\"):\n",
    "        self.headers = {\"storm-api-key\": api_key}\n",
    "        self.api_url = api_url\n",
    "        self.thread_id = None\n",
    "        self.history = []\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"Ask a question in the current thread.\"\"\"\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/api/v2/answer\",\n",
    "            headers=self.headers,\n",
    "            json={\"question\": question}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()[\"data\"]\n",
    "            chat = data[\"chat\"]\n",
    "            \n",
    "            # Store thread ID from first response\n",
    "            if not self.thread_id:\n",
    "                self.thread_id = chat[\"threadId\"]\n",
    "            \n",
    "            # Add to history\n",
    "            self.history.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": chat[\"answer\"],\n",
    "                \"contexts\": len(data[\"contexts\"])\n",
    "            })\n",
    "            \n",
    "            return chat[\"answer\"], data[\"contexts\"]\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Display conversation history.\"\"\"\n",
    "        print(f\"üßµ Thread: {self.thread_id}\\n\")\n",
    "        \n",
    "        for i, turn in enumerate(self.history, 1):\n",
    "            print(f\"Turn {i}:\")\n",
    "            print(f\"  Q: {turn['question']}\")\n",
    "            print(f\"  A: {turn['answer'][:100]}...\" if len(turn['answer']) > 100 else f\"  A: {turn['answer']}\")\n",
    "            print(f\"  üìö {turn['contexts']} sources used\")\n",
    "            print()\n",
    "\n",
    "# Create a chat session\n",
    "session = ChatSession(API_KEY)\n",
    "\n",
    "# Have a conversation\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How does it work?\",\n",
    "    \"What are the benefits?\"\n",
    "]\n",
    "\n",
    "print(\"üí¨ Starting conversation...\\n\")\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer, contexts = session.ask(q)\n",
    "    if answer:\n",
    "        print(f\"A: {answer[:150]}...\" if len(answer) > 150 else f\"A: {answer}\")\n",
    "        print(f\"üìö Sources: {len(contexts)}\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to get answer\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Chat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_context(query, bucket_ids=None, thread_id=None):\n",
    "    \"\"\"Chat with specific buckets and thread context.\"\"\"\n",
    "    \n",
    "    # Note: The /api/v2/answer endpoint doesn't support bucket filtering\n",
    "    # This is shown for the streaming endpoint structure\n",
    "    \n",
    "    print(\"üìù Advanced Chat Parameters:\\n\")\n",
    "    \n",
    "    print(\"1. Thread Management:\")\n",
    "    print(\"   ‚Ä¢ New conversation: Don't specify thread_id\")\n",
    "    print(\"   ‚Ä¢ Continue conversation: Use existing thread_id\")\n",
    "    print(\"   ‚Ä¢ Each thread maintains conversation history\")\n",
    "    \n",
    "    print(\"\\n2. Variables (for template queries):\")\n",
    "    print(\"   ‚Ä¢ Pass dynamic values to queries\")\n",
    "    print(\"   ‚Ä¢ Example: {'product': 'Storm API', 'version': '2.0'}\")\n",
    "    \n",
    "    print(\"\\n3. Answer Structure:\")\n",
    "    print(\"   ‚Ä¢ chat.answer: The AI response\")\n",
    "    print(\"   ‚Ä¢ chat.status: Processing status\")\n",
    "    print(\"   ‚Ä¢ contexts: Source documents used\")\n",
    "    \n",
    "    # Example with variables\n",
    "    query_template = \"Tell me about {topic} in {domain}\"\n",
    "    variables = {\n",
    "        \"topic\": \"document processing\",\n",
    "        \"domain\": \"Storm API\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìã Example with variables:\")\n",
    "    print(f\"Query: {query_template}\")\n",
    "    print(f\"Variables: {variables}\")\n",
    "    \n",
    "    # Note: Variables would be processed by Storm API\n",
    "    # to create the final query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_chat(query, max_retries=3):\n",
    "    \"\"\"Chat with error handling and retries.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{API_URL}/api/v2/answer\",\n",
    "                headers=headers,\n",
    "                json={\"query\": query},\n",
    "                timeout=30  # 30 second timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data[\"status\"] == \"success\":\n",
    "                    return data[\"data\"]\n",
    "                else:\n",
    "                    print(f\"API Error: {data}\")\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                # Rate limited\n",
    "                wait_time = int(response.headers.get(\"Retry-After\", 60))\n",
    "                print(f\"Rate limited. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            elif response.status_code == 401:\n",
    "                print(\"‚ùå Authentication failed. Check API key.\")\n",
    "                return None\n",
    "            \n",
    "            else:\n",
    "                print(f\"HTTP Error {response.status_code}: {response.text}\")\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout on attempt {attempt + 1}\")\n",
    "        \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"Connection error on attempt {attempt + 1}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test edge cases\n",
    "test_queries = [\n",
    "    \"Normal question about Storm API\",\n",
    "    \"\",  # Empty query\n",
    "    \"A\" * 5000,  # Very long query\n",
    "    \"What is ‰ªÄ‰πàÊòØ ü§î?\",  # Unicode and emojis\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing edge cases:\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Test {i}: {query[:50]}...\" if len(query) > 50 else f\"Test {i}: '{query}'\")\n",
    "    \n",
    "    if not query:\n",
    "        print(\"  ‚ö†Ô∏è  Skipping empty query\\n\")\n",
    "        continue\n",
    "    \n",
    "    result = robust_chat(query)\n",
    "    if result:\n",
    "        print(\"  ‚úÖ Success\\n\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Failed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormChatbot:\n",
    "    \"\"\"Interactive chatbot using Storm API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, api_url=\"https://https://live-stargate.sionic.im\"):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = api_url\n",
    "        self.headers = {\"storm-api-key\": api_key}\n",
    "        self.thread_id = None\n",
    "    \n",
    "    def chat(self, message):\n",
    "        \"\"\"Send message and get response.\"\"\"\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/api/v2/answer\",\n",
    "            headers=self.headers,\n",
    "            json={\"query\": message}\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()[\"data\"]\n",
    "            chat = data[\"chat\"]\n",
    "            contexts = data[\"contexts\"]\n",
    "            \n",
    "            # Store thread ID\n",
    "            if not self.thread_id:\n",
    "                self.thread_id = chat[\"threadId\"]\n",
    "            \n",
    "            return {\n",
    "                \"answer\": chat[\"answer\"],\n",
    "                \"sources\": len(contexts),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"answer\": \"Sorry, I couldn't process your request.\",\n",
    "                \"sources\": 0,\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    def format_response(self, response):\n",
    "        \"\"\"Format response for display.\"\"\"\n",
    "        answer = response[\"answer\"]\n",
    "        sources = response[\"sources\"]\n",
    "        \n",
    "        # Add source indicator\n",
    "        if sources > 0:\n",
    "            footer = f\"\\n\\nüìö Based on {sources} sources\"\n",
    "        else:\n",
    "            footer = \"\\n\\nüí≠ General response\"\n",
    "        \n",
    "        return answer + footer\n",
    "\n",
    "# Create chatbot\n",
    "bot = StormChatbot(API_KEY)\n",
    "\n",
    "# Simulate conversation\n",
    "print(\"ü§ñ Storm Chatbot Demo\\n\")\n",
    "print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "# Demo conversation\n",
    "demo_messages = [\n",
    "    \"Hello! What can you help me with?\",\n",
    "    \"Tell me about document processing\",\n",
    "    \"How accurate is the RAG system?\"\n",
    "]\n",
    "\n",
    "for msg in demo_messages:\n",
    "    print(f\"You: {msg}\")\n",
    "    response = bot.chat(msg)\n",
    "    print(f\"Bot: {bot.format_response(response)}\")\n",
    "    print()\n",
    "\n",
    "# Interactive mode (commented out for notebook)\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == 'quit':\n",
    "#         break\n",
    "#     \n",
    "#     response = bot.chat(user_input)\n",
    "#     print(f\"Bot: {bot.format_response(response)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Chat API Best Practices:\\n\")\n",
    "\n",
    "print(\"1. Query Design:\")\n",
    "print(\"   ‚Ä¢ Be specific and clear\")\n",
    "print(\"   ‚Ä¢ Provide context when needed\")\n",
    "print(\"   ‚Ä¢ Avoid overly broad questions\")\n",
    "print(\"   ‚Ä¢ Use natural language\")\n",
    "\n",
    "print(\"\\n2. Thread Management:\")\n",
    "print(\"   ‚Ä¢ Use threads for related questions\")\n",
    "print(\"   ‚Ä¢ Start new threads for new topics\")\n",
    "print(\"   ‚Ä¢ Don't mix unrelated conversations\")\n",
    "\n",
    "print(\"\\n3. Performance:\")\n",
    "print(\"   ‚Ä¢ Cache frequently asked questions\")\n",
    "print(\"   ‚Ä¢ Implement client-side rate limiting\")\n",
    "print(\"   ‚Ä¢ Use appropriate timeouts\")\n",
    "\n",
    "print(\"\\n4. Error Handling:\")\n",
    "print(\"   ‚Ä¢ Always check response status\")\n",
    "print(\"   ‚Ä¢ Provide fallback responses\")\n",
    "print(\"   ‚Ä¢ Log errors for debugging\")\n",
    "\n",
    "# Example: Well-structured query patterns\n",
    "good_queries = [\n",
    "    \"What are the key features of Storm API?\",\n",
    "    \"How do I upload documents using the Python SDK?\",\n",
    "    \"Explain the difference between DEFAULT and STORM_PARSE parsers\",\n",
    "    \"What are the rate limits for the chat API?\"\n",
    "]\n",
    "\n",
    "poor_queries = [\n",
    "    \"Tell me everything\",\n",
    "    \"API?\",\n",
    "    \"How?\",\n",
    "    \"Explain all features and capabilities and use cases\"\n",
    "]\n",
    "\n",
    "print(\"\\n‚úÖ Good Query Examples:\")\n",
    "for q in good_queries:\n",
    "    print(f\"   ‚Ä¢ {q}\")\n",
    "\n",
    "print(\"\\n‚ùå Poor Query Examples:\")\n",
    "for q in poor_queries:\n",
    "    print(f\"   ‚Ä¢ {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Send chat requests to Storm API\n",
    "- ‚úÖ Understand context sources\n",
    "- ‚úÖ Manage conversation threads\n",
    "- ‚úÖ Handle errors properly\n",
    "- ‚úÖ Build a simple chatbot\n",
    "- ‚úÖ Follow best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Streaming Chat](./02-streaming-chat.ipynb) - Real-time streaming responses\n",
    "- [Context Search](./03-context-search.ipynb) - Search for relevant contexts\n",
    "- [Building a Chatbot](./04-chatbot-example.ipynb) - Complete chatbot example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
